# -*- coding: utf-8 -*-
"""Transfer Learning with VGG & ResNet and Cats vs. Dogs dataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OyuX3gDag4AA14F3C0qRoZxacA6DyYbo

Transfer Learning with VGG & ResNet and Cats vs. Dogs dataset

1. Prepare & Explore Dataset
"""

from google.colab import drive
drive.mount('/content/drive')

from tensorflow.keras.preprocessing import image_dataset_from_directory
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization
from tensorflow.keras.models import Sequential
from tensorflow.keras.applications import VGG19, ResNet50
from tensorflow.keras.optimizers import SGD, Adam

# Define the paths to your image directories
train_dir = '/content/drive/MyDrive/train/'
train_dogs_dir = '/content/drive/MyDrive/train/dogs'
train_cats_dir = '/content/drive/MyDrive/train/cats'

# Define data augmentation
datagen = ImageDataGenerator(
    rescale=1./255,  # Rescale pixel values to [0, 1]
    rotation_range=20,  # Randomly rotate images
    width_shift_range=0.2,  # Randomly shift images horizontally
    height_shift_range=0.2,  # Randomly shift images vertically
    horizontal_flip=True,  # Randomly flip images horizontally
    zoom_range=0.2  # Randomly zoom in on images
)

# Create training dataset
train_ds = image_dataset_from_directory(
    directory=train_dir,
    labels='inferred',
    label_mode='categorical',
    batch_size=32,
    image_size=(160, 160),
    validation_split=0.1,
    subset="training",
    seed=1024
)

# Create validation dataset
val_ds = image_dataset_from_directory(
    directory=train_dir,
    labels='inferred',
    label_mode='categorical',
    batch_size=32,
    image_size=(160, 160),
    validation_split=0.1,
    subset="validation",
    seed=1024
)

class_names = val_ds.class_names

class_names

print(train_ds)
print(val_ds)
print(class_names)

#Visualize the data

import matplotlib.pyplot as plt

plt.figure(figsize=(10, 10))
for images, labels in train_ds.take(1):
    for i in range(9):
        ax = plt.subplot(3, 3, i + 1)
        plt.imshow(images[i].numpy().astype("uint8"))
        plt.title(int(list(labels[i])[0]))
        plt.axis("off")

"""2. Define the neural network architecture"""

# define the CNN model
'The first base model used is VGG19. The pretrained weights from the imagenet challenge are used'
base_model_1 = VGG19(include_top=False, weights='imagenet', input_shape=(160,160,3), classes=class_names)

#Lets add the final layers to these base models where the actual classification is done in the dense layers
model_1 = Sequential()
model_1.add(base_model_1)
model_1.add(Flatten())

model_1.summary()

#Add the Dense layers along with activation and batch normalization
model_1.add(Dense(1024,activation=('relu'),input_dim=512))
model_1.add(BatchNormalization())
model_1.add(Dense(512,activation=('relu')))
model_1.add(BatchNormalization())
model_1.add(Dense(256,activation=('relu')))
model_1.add(BatchNormalization())
model_1.add(Dropout(.3))#Adding a dropout layer that will randomly drop 30% of the weights
model_1.add(Dense(128,activation=('relu')))
model_1.add(BatchNormalization())
model_1.add(Dropout(.2))
model_1.add(Dense(2, activation=('softmax'))) #This is the classification layer

#Check final model summary
model_1.summary()

"""3. Compile the neural net"""

# compile your model
model_1.compile(loss='binary_crossentropy', optimizer=Adam(1e-3), metrics=['accuracy'])

train_ds = train_ds.prefetch(buffer_size=32)
val_ds = val_ds.prefetch(buffer_size=32)

"""4. Fit / train the neural net"""

import keras
callbacks = [keras.callbacks.ModelCheckpoint("save_at_{epoch}.h5"),]
model_1.fit(train_ds, validation_data = val_ds, callbacks=callbacks, epochs = 1)

"""5. Evaluate the neural net"""

score = model_1.evaluate(val_ds, verbose=1)
print('Validation accuracy:', score[1])

"""Define model_2 using ResNet"""

# define the CNN model
'For the 2nd base model we will use Resnet 50 and compare the performance against the previous one'
'The hypothesis is that Resnet 50 should perform better because of its deeper architecture'
base_model_2 = ResNet50(include_top=False, weights='imagenet', input_shape=(160,160,3), classes=class_names)

#Lets add the final layers to these base models where the actual classification is done in the dense layers
model_2 = Sequential()
model_2.add(base_model_2)
model_2.add(Flatten())

#Add the Dense layers along with activation and batch normalization
model_2.add(Dense(1024, activation=('relu'), input_dim=512))
model_2.add(BatchNormalization())
model_2.add(Dense(512,activation=('relu')))
model_2.add(BatchNormalization())
model_2.add(Dense(256,activation=('relu')))
model_2.add(BatchNormalization())
model_2.add(Dropout(.3))
model_2.add(Dense(128,activation=('relu')))
model_2.add(BatchNormalization())
model_2.add(Dropout(.2))
model_2.add(Dense(2, activation=('softmax'))) #This is the classification layer

model_2.summary()

model_2.compile(loss='binary_crossentropy', optimizer=Adam(1e-3), metrics=['accuracy'])
callbacks = [keras.callbacks.ModelCheckpoint("model_2_save_at_{epoch}.h5"),]
model_2.fit(train_ds, validation_data = val_ds, callbacks=callbacks, epochs = 1)

score = model_2.evaluate(val_ds, verbose=1)
print('Validation accuracy:', score[1])